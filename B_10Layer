import os
import random
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow import keras
from tensorflow.keras import layers, regularizers
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import GridSearchCV

# ------------------------------------------------------
# 1. Seeds (Optional) & Data Loading
# ------------------------------------------------------
SEED = 42
os.environ["TF_DETERMINISTIC_OPS"] = "1"
random.seed(SEED)
np.random.seed(SEED)
tf.random.set_seed(SEED)

# Load BloodMNIST or similar data
data_dir= r'C:\Users\SIMON\Desktop\ELEC0134-AMLS\bloodmnist.npz'
data = np.load(data_dir)
x_train = data["train_images"]   # (11959, 28, 28)
y_train = data["train_labels"]
x_val   = data["val_images"]
y_val   = data["val_labels"]
x_test  = data["test_images"]
y_test  = data["test_labels"]

# Normalize to [0,1]
x_train = x_train.astype("float32") / 255.0
x_val   = x_val.astype("float32") / 255.0
x_test  = x_test.astype("float32") / 255.0

# Ensure labels are int
y_train = y_train.astype("int")
y_val   = y_val.astype("int")
y_test  = y_test.astype("int")

# Expand dims if grayscale => (batch, 28, 28, 1)
x_train = np.expand_dims(x_train, axis=-1)
x_val   = np.expand_dims(x_val,   axis=-1)
x_test  = np.expand_dims(x_test,  axis=-1)

# ------------------------------------------------------
# 2. Build a 10-Layer CNN (Avoid Negative Dimension)
# ------------------------------------------------------
# We'll do 5 blocks x 2 convs each = 10 conv layers.
# We'll only apply MaxPooling2D in the first 4 blocks, skipping it in the 5th.
def build_cnn_10_layers():
    l2_reg = regularizers.l2(1e-4)
    
    inputs = keras.Input(shape=(28, 28, 3))
    x = inputs

    filters = 32
    num_blocks = 5
    for block_idx in range(num_blocks):
        # Two Convs per block
        for conv_idx in range(2):
            x = layers.Conv2D(
                filters=filters,
                kernel_size=(3, 3),
                padding="same",
                activation="relu",
                kernel_regularizer=l2_reg
            )(x)
            x = layers.BatchNormalization()(x)
        
        # Apply MaxPooling2D + Dropout only in first 4 blocks
        if block_idx < num_blocks - 1:
            x = layers.MaxPooling2D()(x)
            x = layers.Dropout(0.2)(x)
        
        filters *= 2  # Double filters for next block

    # After the 5th block, no additional pooling, so we proceed to flatten
    x = layers.Dropout(0.4)(x)  # optional dropout before flatten
    x = layers.Flatten()(x)
    x = layers.Dense(128, activation="relu", kernel_regularizer=l2_reg)(x)
    x = layers.BatchNormalization()(x)
    x = layers.Dropout(0.5)(x)
    
    outputs = layers.Dense(8, activation="softmax")(x)
    
    model = keras.Model(inputs, outputs)
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-3),
        loss="sparse_categorical_crossentropy",
        metrics=["accuracy"]
    )
    return model

model = build_cnn_10_layers()
model.summary()

# ------------------------------------------------------
# 3. Train the 10-Layer CNN & Check Overfitting
# ------------------------------------------------------
callbacks = [
    keras.callbacks.EarlyStopping(patience=6, restore_best_weights=True, verbose=1),
    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=3, verbose=1)
]

history = model.fit(
    x_train, y_train,
    validation_data=(x_val, y_val),
    epochs=50,
    batch_size=64,
    callbacks=callbacks,
    shuffle=True
)

# Plot training curves to see if there's overfitting
train_acc = history.history["accuracy"]
val_acc   = history.history["val_accuracy"]
train_loss = history.history["loss"]
val_loss   = history.history["val_loss"]

epochs_range = range(1, len(train_acc) + 1)

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(epochs_range, train_acc, 'b-o', label="Train Acc")
plt.plot(epochs_range, val_acc, 'r-o', label="Val Acc")
plt.title("10-Layer CNN Accuracy")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.legend()

plt.subplot(1,2,2)
plt.plot(epochs_range, train_loss, 'b-o', label="Train Loss")
plt.plot(epochs_range, val_loss, 'r-o', label="Val Loss")
plt.title("10-Layer CNN Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend()

plt.tight_layout()
plt.show()

# Evaluate on test set
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)
print(f"[10-Layer CNN] Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}")

# ------------------------------------------------------
# 4. CNN Feature Extraction + SVM (with different kernels)
# ------------------------------------------------------
# We'll extract from the second-last layer (Dense(128))
feature_extractor = keras.Model(
    inputs=model.input,
    outputs=model.layers[-3].output  # The Dense(128) is the third-last layer now
)

train_features = feature_extractor.predict(x_train)
val_features   = feature_extractor.predict(x_val)
test_features  = feature_extractor.predict(x_test)

# Scale features
scaler = StandardScaler()
train_features = scaler.fit_transform(train_features)
val_features   = scaler.transform(val_features)
test_features  = scaler.transform(test_features)

# Grid search for best kernel
svm = SVC(probability=True, random_state=SEED)
param_grid = {
    "C": [0.1, 1, 10],
    "kernel": ["linear", "rbf", "poly", "sigmoid"]
}

grid_search = GridSearchCV(
    svm,
    param_grid,
    scoring="accuracy",
    cv=3,
    verbose=2,
    n_jobs=-1
)

grid_search.fit(train_features, y_train)
print("Best Params for SVM:", grid_search.best_params_)

# Evaluate best SVM on validation & test
best_svm = grid_search.best_estimator_
val_acc_svm = best_svm.score(val_features, y_val)
test_acc_svm = best_svm.score(test_features, y_test)
print(f"[SVM with best kernel] Val Acc: {val_acc_svm:.4f}, Test Acc: {test_acc_svm:.4f}")
